{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e497a-9649-4d0f-be90-1e866d8f5a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca777e8b-4bd5-454e-8c4c-5f85823f3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import numpy as np\n",
    "import wget\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import whisper\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.asr.models import ClusteringDiarizer, NeuralDiarizer\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58fc9ff-919a-4d30-aa8f-cb5a36048a3c",
   "metadata": {},
   "source": [
    "### Main Transcription Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b21019-26ab-42dd-b1d2-8548561e538f",
   "metadata": {},
   "source": [
    "FFmpeg is used to extract the audio from the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7541300f-5a77-460d-88dd-6f4bc27ae4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(input_video, output_audio, start_time='00:00:00', duration='00:15:00'):\n",
    "    # Check if output file exists and remove it\n",
    "    if os.path.exists(output_audio):\n",
    "        os.remove(output_audio)\n",
    "\n",
    "    # Use ffmpeg to extract the audio\n",
    "    try:\n",
    "        ffmpeg.input(input_video, ss=start_time, t=duration).output(output_audio, qscale=0, ar=16000, ac=1).run(overwrite_output=True, capture_stdout=True)\n",
    "    except ffmpeg.Error as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de494d0-0cb9-49c7-9cc8-56adda2433f9",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is used to transcribe the audio into text, the transcribe_section function takes a segment of audio and transcribes the segment into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bcb573-f5b6-41cc-9f6e-e8b39b3df31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base.en\")  # Load the Whisper model in English with the \"base.en\" configuration\n",
    "\n",
    "# Function to extract and transcribe a section of audio\n",
    "def transcribe_section(audio, start_time, end_time, threshold):\n",
    "    # Extract the section from the audio based on start and end times\n",
    "    section = audio[start_time*1000:end_time*1000]  # Convert times from seconds to milliseconds\n",
    "    \n",
    "    # Export the audio section to a temporary file\n",
    "    temp_file = \"temp_section.wav\"  # Temporary file name\n",
    "    section.export(temp_file, format=\"wav\")  # Save the section as a .wav file for transcription\n",
    "    \n",
    "    try:\n",
    "        # Transcribe the temporary file using Whisper\n",
    "        result = model.transcribe(temp_file)\n",
    "        \n",
    "        # Initialize an empty string to hold the filtered transcription\n",
    "        filtered_text = \"\"\n",
    "        \n",
    "        # If there are segments in the transcription result, process each segment\n",
    "        if len(result[\"segments\"]) > 0:\n",
    "            for segment in result[\"segments\"]:\n",
    "                # Only include text from segments with a no-speech probability below the threshold\n",
    "                if segment[\"no_speech_prob\"] < threshold:\n",
    "                    filtered_text += segment[\"text\"]  # Append the transcribed text to the output\n",
    "                \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Handle errors from subprocess (e.g., issues with audio extraction)\n",
    "        return ''\n",
    "    except RuntimeError as e:\n",
    "        # Print a runtime error message and return an empty string if a RuntimeError occurs\n",
    "        print(f\"RuntimeError: Error processing section from {start_time} to {end_time}: {e}\")\n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        # Print a general error message for any other exceptions\n",
    "        print(f\"Unexpected error: Error processing section from {start_time} to {end_time}: {e}\")\n",
    "        return ''\n",
    "\n",
    "    return filtered_text  # Return the filtered transcription text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80c050-5401-4571-8be4-b23e374a6a4c",
   "metadata": {},
   "source": [
    "We only want to process mp4 files, so this function checks to see if the files are mp4 files, or start with \"._\", which are metadata files generated on macOS which we don't want to process.\n",
    "\n",
    "The single parameter is used to select a single file for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4471329-21a1-4cf2-8523-70ddd7fb20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file should be processed\n",
    "def should_process_file(file_path, existing_files, single):\n",
    "    file_name = file_path.stem\n",
    "    if file_name in existing_files or \\\n",
    "       (file_path.suffix not in ['.mp4', '.MP4']) or \\\n",
    "       file_name.startswith('._') or \\\n",
    "       (single is not None and file_name not in single):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabfd71-9595-41ff-ac84-b94cabe293e9",
   "metadata": {},
   "source": [
    "This section uses NVIDIA's NeMo diarization model to diarize the extracted audio. Diarization is the process of identifying different speakers in audio, which is necessary to produce accurate transcriptions. The hyperparameters were chosen by following the guide found here: https://github.com/NVIDIA/NeMo/blob/main/tutorials/speaker_tasks/Speaker_Diarization_Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7228b1c-465e-4f8f-9a21-c8a95de56973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diarize the audio to separate speakers\n",
    "def diarize_audio(output_audio, model_config_url=\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_telephonic.yaml\"):\n",
    "    # Prepare metadata for NeMo diarization\n",
    "    meta = {'audio_filepath': output_audio, 'offset': 0, 'duration': None, 'label': 'infer', 'text': '-', 'num_speakers': None, 'rttm_filepath': None, 'uem_filepath': None}\n",
    "    with open(\"input_manifest.json\", 'w', encoding='utf-8') as fp:\n",
    "        json.dump(meta, fp)\n",
    "        fp.write('\\n')\n",
    "\n",
    "    # Load or download diarizer configuration\n",
    "    model_config_path = \"model_data/diar_infer_telephonic.yaml\"\n",
    "    if not os.path.exists(model_config_path):\n",
    "        os.makedirs(\"model_data\", exist_ok=True)\n",
    "        model_config_path = wget.download(model_config_url, \"model_data\")\n",
    "    \n",
    "    # Set configuration parameters\n",
    "    config = OmegaConf.load(model_config_path)\n",
    "    config.diarizer.manifest_filepath = \"input_manifest.json\"\n",
    "    config.diarizer.out_dir = \"oracle_vad\"\n",
    "    config.device = \"cpu\" # CHANGE THIS TO cuda TO USE NVIDIA GPU\n",
    "    config.num_workers = 0\n",
    "    \n",
    "    # Initialize and run the diarization model\n",
    "    diarizer_model = NeuralDiarizer(cfg=config)\n",
    "    diarizer_model.diarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec4030-fa64-453e-a69b-83b480c9ac37",
   "metadata": {},
   "source": [
    "The output of the diarization is a rttm file which we use to extract the audio segments attributed to each speaker.\n",
    "\n",
    "The threshold paremeter is used to ignore sections that are shorter than the threshold. The default value is 0.5, which ignores all audio sections that are shorter than 0.5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43985325-d911-488f-b882-db7001d0af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and aggregate speaker segments\n",
    "def get_speaker_segments(rttm_path, time_threshold=0.5):\n",
    "    columns = ['type', 'file_id', 'channel_id', 'begin_time', 'duration', 'ortho', 'speaker_type', 'speaker_name', 'confidence_score', 'signal_lookahead']\n",
    "    rttm_df = pd.read_csv(rttm_path, delim_whitespace=True, names=columns, comment='#')\n",
    "\n",
    "    audio_sections = []\n",
    "    current_section = None\n",
    "    for _, row in rttm_df.iterrows():\n",
    "        begin_time, end_time, speaker = row[\"begin_time\"], row[\"begin_time\"] + row[\"duration\"], row[\"speaker_name\"]\n",
    "        if row[\"duration\"] > time_threshold:\n",
    "            if current_section is None or current_section[2] != speaker or begin_time - current_section[1] >= 1:\n",
    "                if current_section:\n",
    "                    audio_sections.append(current_section)\n",
    "                current_section = [begin_time, end_time, speaker]\n",
    "            else:\n",
    "                current_section[1] = end_time\n",
    "    if current_section:\n",
    "        audio_sections.append(current_section)\n",
    "\n",
    "    return audio_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840890b-6dca-4c26-af53-00138a6e35ef",
   "metadata": {},
   "source": [
    "With the extracted segments, we can the transcribe each section using Whisper.\n",
    "\n",
    "Whisper outputs a value that represents the percent chance the section isn't speech. The threshold parameter ignores sections where this chance is above the threshold. By default it is set to 0.9, which ignores all segements with a 90% or higher chance to not be speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39ba98e-49b9-498c-a149-563d89689307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe each speaker section\n",
    "def transcribe_sections(audio, audio_sections, prob_threshold=0.9):\n",
    "    transcriptions = []\n",
    "    first_speech = audio_sections[0][0] if audio_sections else 0\n",
    "\n",
    "    for start_time, end_time, speaker in audio_sections:\n",
    "        text = transcribe_section(audio, start_time, end_time, prob_threshold)\n",
    "        if text:\n",
    "            transcriptions.append([start_time - first_speech, end_time - first_speech, speaker, text])\n",
    "    return pd.DataFrame(transcriptions, columns=[\"Start Time\", \"End Time\", \"Speaker\", \"Transcription\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285a574-1c19-4b6a-b836-cef2a9c81011",
   "metadata": {},
   "source": [
    "Once the transcriptions are complete, we remove the noise at the beginning of the transcription and identify the 2 most prominent speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1738bfc1-4be6-447d-9864-4a86c84accad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and label speakers in transcriptions\n",
    "def filter_and_label_speakers(df):\n",
    "    # In some studies, the RA says \"You will have an 8 minute conversation/chat\", we want to look for this and remove all of the transcriptions before it\n",
    "    df['match_count'] = df['Transcription'].apply(lambda row: count_matches(row, [\"8\", \"eight\", \"minute\", \"conversation\", \"chat\"]))\n",
    "    index_RA = df.index.get_loc(df['match_count'].idxmax())\n",
    "    filtered_df = df.iloc[index_RA + 1:][[\"Start Time\", \"End Time\", \"Speaker\", \"Transcription\"]] if df.iloc[index_RA][\"Start Time\"] / df.iloc[-1][\"Start Time\"] < 0.5 and df['match_count'].max() > 1 else df\n",
    "\n",
    "    top_2_speakers = filtered_df['Speaker'].value_counts().nlargest(2).index.tolist()\n",
    "    replace_map = {top_2_speakers[0]: \"Speaker 1\", top_2_speakers[1]: \"Speaker 2\"} if len(top_2_speakers) > 1 else {top_2_speakers[0]: \"Speaker 1\"}\n",
    "    return filtered_df.replace({\"Speaker\": replace_map})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a262cd-b6bc-4b40-ab24-a00f8403c9ce",
   "metadata": {},
   "source": [
    "This function counts the instances of a list of words in the transcription lines. This is used to find the phrase \"8 minute conversation\", which is used to remove the beginning of the transcription that doesn't involve the participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec652b28-c94a-4e7b-952b-5261bdf4afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(row, words):\n",
    "    return sum(word in row for word in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724383a3-60f7-48fd-a296-95163041c388",
   "metadata": {},
   "source": [
    "We then save the transcriptions after splitting the 2 most prominent speakers, which allow us to produce both the \"dyad\" files which include both speakers as well as \"single\" files which include only 1 speaker. We also include a \"full\" file which are the unfiltered transcriptions, this allows us to regenerate the \"dyad\" and \"single\" files without having to rerun the entire pipeline.\n",
    "\n",
    "The method paramter selects whether the pipeline should treat the input file as having 2 speakers [method == \"Dyad\"], or only 1 speaker [method == \"Single\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7cfe295-2be0-4b71-bc31-daa44b1fea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creation_time(file_path):\n",
    "    '''\n",
    "    Getting file Metadata if exists\n",
    "    '''\n",
    "    cmd = [\n",
    "        'ffprobe',\n",
    "        '-v', 'quiet',\n",
    "        '-show_entries', 'format_tags=creation_time',\n",
    "        '-of', 'default=noprint_wrappers=1:nokey=1',\n",
    "        str(file_path)\n",
    "    ]\n",
    "    try:\n",
    "        output = subprocess.check_output(cmd, stderr=subprocess.DEVNULL).decode().strip()\n",
    "        if output:\n",
    "            output = output.replace(\"T\", \" \").split(\".\")[0]\n",
    "            return output\n",
    "    except subprocess.CalledProcessError:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435469e5-a944-4d61-8690-9b810b93b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcriptions(df, output_directory, file_path, method=\"dyad\"):\n",
    "    # Create required directories if they don't exist\n",
    "    os.makedirs(os.path.join(output_directory, \"dyad\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_directory, \"single\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_directory, \"full\"), exist_ok=True)\n",
    "\n",
    "    \n",
    "    def format_duration(seconds):\n",
    "        seconds = int(round(seconds))\n",
    "        h = seconds // 3600\n",
    "        m = (seconds % 3600) // 60\n",
    "        s = seconds % 60\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    def seconds_to_hms(seconds):\n",
    "        ms = int((seconds % 1) * 1000)\n",
    "        h  = int(seconds // 3600)\n",
    "        m  = int((seconds % 3600) // 60)\n",
    "        s  = int(seconds % 60)\n",
    "\n",
    "        return f\"{h:02}:{m:02}:{s:02}.{ms:03}\"\n",
    "\n",
    "    def fill_meta_data():\n",
    "        title = file_name\n",
    "\n",
    "        creation_time = get_creation_time(file_path)\n",
    "        if not creation_time:\n",
    "            file_stats = file_path.stat()\n",
    "            creation_time = datetime.datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "        duration_sec = df[\"End Time\"].max() - df[\"Start Time\"].min()\n",
    "        duration_str = format_duration(duration_sec)\n",
    "\n",
    "        meta_lines = [\n",
    "            f\"# Title: {title}\",\n",
    "            f\"# Date Modified: {creation_time}\",\n",
    "            f\"# Duration: {duration_str}\",\n",
    "            f\"# Number of Speakers: {df['Speaker'].nunique()}\",\n",
    "            f\"# Number of Segments: {len(df)}\",\n",
    "            f\"# Format: Start Time | End Time | Speaker | Transcription\",\n",
    "            f\"# Time Format: hh:mm:ss.mmm (hours:minutes:seconds.milliseconds)\",\n",
    "            f\"# ---------------------------------------------\"\n",
    "            \"\\n\"\n",
    "        ]\n",
    "\n",
    "        return meta_lines\n",
    "\n",
    "    def write_file(df_subset, out_path):\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            for line in meta_lines:\n",
    "                f.write(line+'\\n')\n",
    "            df_subset.to_csv(f, sep='|', index=False, header=False)\n",
    "    \n",
    "    # Include meta data here\n",
    "    file_name = file_path.stem\n",
    "    meta_lines = fill_meta_data()\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Start Time\"] = df[\"Start Time\"].apply(seconds_to_hms)\n",
    "    df[\"End Time\"]   = df[\"End Time\"].apply(seconds_to_hms)\n",
    "    \n",
    "    if method == \"dyad\":\n",
    "        write_file(\n",
    "            df[(df[\"Speaker\"] == \"Speaker 1\") | (df[\"Speaker\"] == \"Speaker 2\")],\n",
    "            os.path.join(output_directory, \"dyad\", file_name + '_dyad.txt')\n",
    "        )\n",
    "        write_file(\n",
    "            df[df[\"Speaker\"] == \"Speaker 1\"],\n",
    "            os.path.join(output_directory, \"single\", file_name + '_single_X.txt')\n",
    "        )\n",
    "        write_file(\n",
    "            df[df[\"Speaker\"] == \"Speaker 2\"],\n",
    "            os.path.join(output_directory, \"single\", file_name + '_single_Y.txt')\n",
    "        )\n",
    "    elif \"single\":\n",
    "        write_file(\n",
    "            df[df[\"Speaker\"] == \"Speaker 1\"],\n",
    "            os.path.join(output_directory, \"single\", file_name + '_single.txt')\n",
    "        )\n",
    "\n",
    "\n",
    "    write_file(\n",
    "        df,\n",
    "        os.path.join(output_directory, 'full', file_name + '_full.txt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee886c19-1808-49e8-bf0e-5d07afb16839",
   "metadata": {},
   "source": [
    "This function runs the full pipeline on one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3cfd3f9-1bbb-46bd-aace-d30126b4e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(file_path, output_directory, prob_threshold=0.9, method=\"dyad\"):\n",
    "    # Ensure file_path is a Path object\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Process a single video file through the pipeline\n",
    "    print(f\"Processing file: {file_path.stem}\")\n",
    "    workflow_start_time = time.time()\n",
    "    \n",
    "    # Step 1: Extract Audio\n",
    "    output_audio = 'temp_extracted_audio.wav'\n",
    "    extract_audio(file_path, output_audio)\n",
    "\n",
    "    # Step 2: Diarize\n",
    "    diarize_audio(output_audio)\n",
    "    \n",
    "    # Step 3: Get Speaker Segments\n",
    "    audio_sections = get_speaker_segments('oracle_vad/pred_rttms/temp_extracted_audio.rttm')\n",
    "    \n",
    "    # Step 4: Transcribe Sections\n",
    "    audio = AudioSegment.from_wav(output_audio)\n",
    "    df = transcribe_sections(audio, audio_sections, prob_threshold)\n",
    "    \n",
    "    # Step 5: Filter and Label Speakers\n",
    "    filtered_df = filter_and_label_speakers(df)\n",
    "    \n",
    "    # Step 6: Save Transcriptions\n",
    "    save_transcriptions(filtered_df, output_directory, file_path, method)\n",
    "    \n",
    "    # Clean up temporary audio file\n",
    "    # os.remove(output_audio) if os.path.exists(output_audio) else None\n",
    "    print(f\"Elapsed time: {round(time.time() - workflow_start_time, 2)} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf18e29-c474-43a6-8377-1dc64fd3f961",
   "metadata": {},
   "source": [
    "### Below are functions used to navigate through directories and call process_single_video, you will likely have to modify it to suit your project structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47eaa605-33a3-4330-92e2-0383ba2bf57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_filename(filename):\n",
    "    return re.match(r\"^[A-Z]{3,4}\\d{2}.*\\.mp4$\", filename, re.IGNORECASE) is not None\n",
    "\n",
    "def run_in_folder(input_directory, output_directory, threshold=0.9, method=\"dyad\"):\n",
    "    directory_path = Path(input_directory)\n",
    "    output_directory_path = Path(output_directory)\n",
    "\n",
    "    # Ensure required subdirectories exist\n",
    "    for sub_folder in [\"dyad\", \"full\", \"single\"]:\n",
    "        (output_directory_path / sub_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check for already processed files\n",
    "    existing_files = [\n",
    "        file.stem.split(\"_\")[0]\n",
    "        for sub_folder in [\"dyad\", \"full\", \"single\"]\n",
    "        for file in (output_directory_path / sub_folder).glob(\"*\")\n",
    "    ]\n",
    "\n",
    "    for entry in directory_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix.lower() == \".mp4\" and is_valid_filename(entry.name):\n",
    "            if entry.stem not in existing_files:\n",
    "                process_single_video(entry, output_directory, threshold, method)\n",
    "\n",
    "        elif entry.is_dir():\n",
    "            for nested_file in entry.glob(\"*.mp4\"):\n",
    "                if is_valid_filename(nested_file.name):\n",
    "                    if nested_file.stem not in existing_files:\n",
    "                        process_single_video(nested_file, output_directory, threshold, method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e3ae9-6e0d-46f7-891a-c2842522c2b2",
   "metadata": {},
   "source": [
    "This function loops through a list of subfolders to execute the main pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a7c79e-9de6-4283-bd67-59cd6df07678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple folders\n",
    "def run_all_folders(input_directory, output_directory, folders=[\"VTV\", \"FTF\", \"VGC\"], methods=[\"dyad\", \"dyad\", \"dyad\"], threshold=0.9):\n",
    "    for folder, method in zip(folders, methods):\n",
    "        run_in_folder(input_directory + \"/\" + folder + \"/\", output_directory + \"/\" + folder + \"/\", single=single, threshold=threshold, method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df8bde-07ec-4ba9-8105-8dc6f5d3b640",
   "metadata": {},
   "source": [
    "Here are example executions of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a80fc829-4be7-45f6-a798-b0b3a2fecc8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: VGC13A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/gnamiro/miniconda3/envs/transcript --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'Data/VGC13A.MP4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2023-11-23T19:23:47.000000Z\n",
      "  Duration: 00:09:56.80, start: 0.000000, bitrate: 445 kb/s\n",
      "    Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 32000 Hz, mono, fltp, 126 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-11-23T19:23:47.000000Z\n",
      "      handler_name    : AAC audio\n",
      "    Stream #0:1(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1280x720, 316 kb/s, 25 fps, 25 tbr, 30k tbn, 60k tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-11-23T19:23:47.000000Z\n",
      "      handler_name    : H.264/AVC video\n",
      "      encoder         : AVC Coding\n",
      "Please use -q:a or -q:v, -qscale is ambiguous\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'temp_extracted_audio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.45.100\n",
      "    Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-11-23T19:23:47.000000Z\n",
      "      handler_name    : AAC audio\n",
      "      encoder         : Lavc58.91.100 pcm_s16le\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:15 msdd_models:1120] Loading pretrained diar_msdd_telephonic model from NGC\n",
      "[NeMo I 2025-05-19 20:59:15 cloud:58] Found existing object /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2025-05-19 20:59:15 cloud:64] Re-using file from: /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
      "[NeMo I 2025-05-19 20:59:15 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size=   18650kB time=00:09:56.80 bitrate= 256.0kbits/s speed=2.04e+03x    \n",
      "video:0kB audio:18650kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000408%\n",
      "[NeMo W 2025-05-19 20:59:16 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: true\n",
      "    \n",
      "[NeMo W 2025-05-19 20:59:16 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2025-05-19 20:59:16 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    seq_eval_mode: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:16 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-19 20:59:17 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-19 20:59:17 save_restore_connector:275] Model EncDecDiarLabelModel was successfully restored from /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2025-05-19 20:59:17 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-19 20:59:17 clustering_diarizer:117] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2025-05-19 20:59:17 cloud:58] Found existing object /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2025-05-19 20:59:17 cloud:64] Re-using file from: /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2025-05-19 20:59:17 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-19 20:59:17 classification_models:641] Please use the EncDecSpeakerLabelModel instead of this model. EncDecClassificationModel model is kept for backward compatibility with older models.\n",
      "[NeMo W 2025-05-19 20:59:17 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-05-19 20:59:17 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-05-19 20:59:17 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:17 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-19 20:59:17 save_restore_connector:275] Model EncDecClassificationModel was successfully restored from /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2025-05-19 20:59:17 msdd_models:892] Multiscale Weights: [1, 1, 1, 1, 1]\n",
      "[NeMo I 2025-05-19 20:59:17 msdd_models:893] Clustering Parameters: {\n",
      "        \"oracle_num_speakers\": false,\n",
      "        \"max_num_speakers\": 8,\n",
      "        \"enhanced_count_thres\": 80,\n",
      "        \"max_rp_threshold\": 0.25,\n",
      "        \"sparse_search_volume\": 30,\n",
      "        \"maj_vote_spk_count\": false,\n",
      "        \"chunk_cluster_count\": 50,\n",
      "        \"embeddings_per_chunk\": 10000\n",
      "    }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-19 20:59:17 clustering_diarizer:398] Deleting previous clustering diarizer outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:17 speaker_utils:92] Number of files to diarize: 1\n",
      "[NeMo I 2025-05-19 20:59:17 clustering_diarizer:303] Split long audio file to avoid CUDA memory issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:25<00:00, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:43 vad_utils:146] The prepared manifest file exists. Overwriting!\n",
      "[NeMo I 2025-05-19 20:59:43 classification_models:594] Perform streaming frame-level VAD\n",
      "[NeMo I 2025-05-19 20:59:43 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-19 20:59:43 collections:880] Dataset successfully loaded with 12 items and total duration provided from manifest is  0.17 hours.\n",
      "[NeMo I 2025-05-19 20:59:43 collections:886] # 12 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "vad: 100%|██████████████████████████████████████████████████████████████████████████████| 12/12 [00:24<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:43 clustering_diarizer:244] Generating predictions with overlapping input segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:45 clustering_diarizer:256] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating speech segments: 100%|███████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 20:59:45 clustering_diarizer:281] Subsegmentation for embedding extraction: scale0, oracle_vad/speaker_outputs/subsegments_scale0.json\n",
      "[NeMo I 2025-05-19 20:59:45 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-19 20:59:45 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-19 20:59:45 collections:880] Dataset successfully loaded with 647 items and total duration provided from manifest is  0.25 hours.\n",
      "[NeMo I 2025-05-19 20:59:45 collections:886] # 647 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 11/11 [04:00<00:00, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:03:46 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-19 21:03:46 clustering_diarizer:281] Subsegmentation for embedding extraction: scale1, oracle_vad/speaker_outputs/subsegments_scale1.json\n",
      "[NeMo I 2025-05-19 21:03:46 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-19 21:03:46 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-19 21:03:46 collections:880] Dataset successfully loaded with 778 items and total duration provided from manifest is  0.26 hours.\n",
      "[NeMo I 2025-05-19 21:03:46 collections:886] # 778 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 13/13 [05:25<00:00, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:09:11 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-19 21:09:11 clustering_diarizer:281] Subsegmentation for embedding extraction: scale2, oracle_vad/speaker_outputs/subsegments_scale2.json\n",
      "[NeMo I 2025-05-19 21:09:11 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-19 21:09:11 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-19 21:09:11 collections:880] Dataset successfully loaded with 976 items and total duration provided from manifest is  0.26 hours.\n",
      "[NeMo I 2025-05-19 21:09:11 collections:886] # 976 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 16/16 [04:55<00:00, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:14:07 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-19 21:14:07 clustering_diarizer:281] Subsegmentation for embedding extraction: scale3, oracle_vad/speaker_outputs/subsegments_scale3.json\n",
      "[NeMo I 2025-05-19 21:14:07 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-19 21:14:07 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-19 21:14:07 collections:880] Dataset successfully loaded with 1313 items and total duration provided from manifest is  0.27 hours.\n",
      "[NeMo I 2025-05-19 21:14:07 collections:886] # 1313 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 21/21 [04:33<00:00, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:18:41 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-19 21:18:41 clustering_diarizer:281] Subsegmentation for embedding extraction: scale4, oracle_vad/speaker_outputs/subsegments_scale4.json\n",
      "[NeMo I 2025-05-19 21:18:41 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-19 21:18:41 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-19 21:18:41 collections:880] Dataset successfully loaded with 2000 items and total duration provided from manifest is  0.27 hours.\n",
      "[NeMo I 2025-05-19 21:18:41 collections:886] # 2000 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 32/32 [06:34<00:00, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:25:16 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-05-19 21:25:16 speaker_utils:473] cuda=False, using CPU for eigen decomposition. This might slow down the clustering process.\n",
      "clustering: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [-1:59:37<00:00, -0.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:24:52 clustering_diarizer:451] Outputs are saved in /home/gnamiro/GSGS/transcription/NemoWhisperTranscriptionPipeline/oracle_vad directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-05-19 21:24:52 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:24:52 msdd_models:988] Loading embedding pickle file of scale:0 at oracle_vad/speaker_outputs/embeddings/subsegments_scale0_embeddings.pkl\n",
      "[NeMo I 2025-05-19 21:24:52 msdd_models:988] Loading embedding pickle file of scale:1 at oracle_vad/speaker_outputs/embeddings/subsegments_scale1_embeddings.pkl\n",
      "[NeMo I 2025-05-19 21:24:52 msdd_models:988] Loading embedding pickle file of scale:2 at oracle_vad/speaker_outputs/embeddings/subsegments_scale2_embeddings.pkl\n",
      "[NeMo I 2025-05-19 21:24:52 msdd_models:988] Loading embedding pickle file of scale:3 at oracle_vad/speaker_outputs/embeddings/subsegments_scale3_embeddings.pkl\n",
      "[NeMo I 2025-05-19 21:24:52 msdd_models:988] Loading embedding pickle file of scale:4 at oracle_vad/speaker_outputs/embeddings/subsegments_scale4_embeddings.pkl\n",
      "[NeMo I 2025-05-19 21:24:52 msdd_models:966] Loading cluster label file from oracle_vad/speaker_outputs/subsegments_scale4_cluster.label\n",
      "[NeMo I 2025-05-19 21:24:52 collections:1212] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2025-05-19 21:24:52 collections:1216] Total 1 session files loaded accounting to # 1 audio clips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:24:52 msdd_models:1444]      [Threshold: 0.7000] [use_clus_as_main=False] [diar_window=50]\n",
      "[NeMo I 2025-05-19 21:24:52 speaker_utils:92] Number of files to diarize: 1\n",
      "[NeMo I 2025-05-19 21:24:52 speaker_utils:92] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-05-19 21:24:52 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:24:52 speaker_utils:92] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-19 21:24:52 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:24:52 speaker_utils:92] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-19 21:24:52 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-19 21:24:52 msdd_models:1473]   \n",
      "    \n",
      "Elapsed time: 1566.76 seconds\n"
     ]
    }
   ],
   "source": [
    "process_single_video(\"./Data/VGC13A.MP4\", \"Test\", prob_threshold=0.9, method=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2888c4d9-b2a9-43b0-9c05-d64dfb3d13a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: FtF32A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/gnamiro/miniconda3/envs/transcript --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Guessed Channel Layout for Input Stream #0.1 : stereo\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'Data/FtF32A.MP4':\n",
      "  Metadata:\n",
      "    major_brand     : XAVC\n",
      "    minor_version   : 16785407\n",
      "    compatible_brands: XAVCmp42iso2\n",
      "    creation_time   : 2023-03-24T15:29:08.000000Z\n",
      "  Duration: 00:10:37.14, start: 0.000000, bitrate: 51771 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709/bt709/iec61966-2-4), 1920x1080 [SAR 1:1 DAR 16:9], 49742 kb/s, 59.94 fps, 59.94 tbr, 60k tbn, 119.88 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-03-24T15:29:08.000000Z\n",
      "      handler_name    : Video Media Handler\n",
      "      encoder         : AVC Coding\n",
      "    Stream #0:1(und): Audio: pcm_s16be (twos / 0x736F7774), 48000 Hz, stereo, s16, 1536 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-03-24T15:29:08.000000Z\n",
      "      handler_name    : Sound Media Handler\n",
      "    Stream #0:2(und): Data: none (rtmd / 0x646D7472), 491 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-03-24T15:29:08.000000Z\n",
      "      handler_name    : Timed Metadata Media Handler\n",
      "      timecode        : 10:38:36:20\n",
      "Please use -q:a or -q:v, -qscale is ambiguous\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (pcm_s16be (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'temp_extracted_audio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : XAVC\n",
      "    minor_version   : 16785407\n",
      "    compatible_brands: XAVCmp42iso2\n",
      "    ISFT            : Lavf58.45.100\n",
      "    Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-03-24T15:29:08.000000Z\n",
      "      handler_name    : Sound Media Handler\n",
      "      encoder         : Lavc58.91.100 pcm_s16le\n",
      "size=   19911kB time=00:10:37.13 bitrate= 256.0kbits/s speed=2.52e+03x    \n",
      "video:0kB audio:19911kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000383%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [................................................................................] 7646 / 7646[NeMo I 2025-05-14 21:12:38 msdd_models:1120] Loading pretrained diar_msdd_telephonic model from NGC\n",
      "[NeMo I 2025-05-14 21:12:38 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/diar_msdd_telephonic/versions/1.0.1/files/diar_msdd_telephonic.nemo to /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
      "100% [......................................................................] 107609008 / 107609008[NeMo I 2025-05-14 21:12:42 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-14 21:12:44 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: true\n",
      "    \n",
      "[NeMo W 2025-05-14 21:12:44 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2025-05-14 21:12:44 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    seq_eval_mode: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:12:44 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-14 21:12:44 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-14 21:12:45 save_restore_connector:275] Model EncDecDiarLabelModel was successfully restored from /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2025-05-14 21:12:45 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-14 21:12:45 clustering_diarizer:117] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2025-05-14 21:12:45 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/vad_multilingual_marblenet/versions/1.10.0/files/vad_multilingual_marblenet.nemo to /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "100% [............................................................................] 501760 / 501760[NeMo I 2025-05-14 21:12:46 common:826] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-14 21:12:46 classification_models:641] Please use the EncDecSpeakerLabelModel instead of this model. EncDecClassificationModel model is kept for backward compatibility with older models.\n",
      "[NeMo W 2025-05-14 21:12:46 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-05-14 21:12:46 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-05-14 21:12:46 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:12:46 features:305] PADDING: 16\n",
      "[NeMo I 2025-05-14 21:12:46 save_restore_connector:275] Model EncDecClassificationModel was successfully restored from /home/gnamiro/.cache/torch/NeMo/NeMo_2.3.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2025-05-14 21:12:46 msdd_models:892] Multiscale Weights: [1, 1, 1, 1, 1]\n",
      "[NeMo I 2025-05-14 21:12:46 msdd_models:893] Clustering Parameters: {\n",
      "        \"oracle_num_speakers\": false,\n",
      "        \"max_num_speakers\": 8,\n",
      "        \"enhanced_count_thres\": 80,\n",
      "        \"max_rp_threshold\": 0.25,\n",
      "        \"sparse_search_volume\": 30,\n",
      "        \"maj_vote_spk_count\": false,\n",
      "        \"chunk_cluster_count\": 50,\n",
      "        \"embeddings_per_chunk\": 10000\n",
      "    }\n",
      "[NeMo I 2025-05-14 21:12:46 speaker_utils:92] Number of files to diarize: 1\n",
      "[NeMo I 2025-05-14 21:12:46 clustering_diarizer:303] Split long audio file to avoid CUDA memory issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:12:55 classification_models:594] Perform streaming frame-level VAD\n",
      "[NeMo I 2025-05-14 21:12:55 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-14 21:12:55 collections:880] Dataset successfully loaded with 13 items and total duration provided from manifest is  0.18 hours.\n",
      "[NeMo I 2025-05-14 21:12:55 collections:886] # 13 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "vad: 100%|██████████████████████████████████████████████████████████████████████████████| 13/13 [00:27<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:13:23 clustering_diarizer:244] Generating predictions with overlapping input segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:13:26 clustering_diarizer:256] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating speech segments: 100%|███████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:13:26 clustering_diarizer:281] Subsegmentation for embedding extraction: scale0, oracle_vad/speaker_outputs/subsegments_scale0.json\n",
      "[NeMo I 2025-05-14 21:13:26 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-14 21:13:26 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-14 21:13:26 collections:880] Dataset successfully loaded with 559 items and total duration provided from manifest is  0.18 hours.\n",
      "[NeMo I 2025-05-14 21:13:26 collections:886] # 559 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] extract embeddings: 100%|███████████████████████████████████████████████████████████| 9/9 [03:42<00:00, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:17:09 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-14 21:17:09 clustering_diarizer:281] Subsegmentation for embedding extraction: scale1, oracle_vad/speaker_outputs/subsegments_scale1.json\n",
      "[NeMo I 2025-05-14 21:17:09 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-14 21:17:09 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-14 21:17:09 collections:880] Dataset successfully loaded with 643 items and total duration provided from manifest is  0.18 hours.\n",
      "[NeMo I 2025-05-14 21:17:09 collections:886] # 643 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 11/11 [05:08<00:00, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:22:17 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-14 21:22:17 clustering_diarizer:281] Subsegmentation for embedding extraction: scale2, oracle_vad/speaker_outputs/subsegments_scale2.json\n",
      "[NeMo I 2025-05-14 21:22:17 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-14 21:22:17 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-14 21:22:17 collections:880] Dataset successfully loaded with 795 items and total duration provided from manifest is  0.19 hours.\n",
      "[NeMo I 2025-05-14 21:22:17 collections:886] # 795 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 13/13 [04:10<00:00, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:26:27 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-14 21:26:27 clustering_diarizer:281] Subsegmentation for embedding extraction: scale3, oracle_vad/speaker_outputs/subsegments_scale3.json\n",
      "[NeMo I 2025-05-14 21:26:27 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-14 21:26:27 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-14 21:26:27 collections:880] Dataset successfully loaded with 1054 items and total duration provided from manifest is  0.20 hours.\n",
      "[NeMo I 2025-05-14 21:26:27 collections:886] # 1054 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 17/17 [04:15<00:00, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:30:43 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n",
      "[NeMo I 2025-05-14 21:30:43 clustering_diarizer:281] Subsegmentation for embedding extraction: scale4, oracle_vad/speaker_outputs/subsegments_scale4.json\n",
      "[NeMo I 2025-05-14 21:30:43 clustering_diarizer:337] Extracting embeddings for Diarization\n",
      "[NeMo I 2025-05-14 21:30:43 collections:879] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2025-05-14 21:30:43 collections:880] Dataset successfully loaded with 1609 items and total duration provided from manifest is  0.21 hours.\n",
      "[NeMo I 2025-05-14 21:30:43 collections:886] # 1609 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] extract embeddings: 100%|█████████████████████████████████████████████████████████| 26/26 [04:59<00:00, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:42 clustering_diarizer:383] Saved embedding files to oracle_vad/speaker_outputs/embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-05-14 21:35:42 speaker_utils:473] cuda=False, using CPU for eigen decomposition. This might slow down the clustering process.\n",
      "clustering: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:43 clustering_diarizer:451] Outputs are saved in /home/gnamiro/GSGS/transcription/NemoWhisperTranscriptionPipeline/oracle_vad directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-05-14 21:35:43 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:43 msdd_models:988] Loading embedding pickle file of scale:0 at oracle_vad/speaker_outputs/embeddings/subsegments_scale0_embeddings.pkl\n",
      "[NeMo I 2025-05-14 21:35:43 msdd_models:988] Loading embedding pickle file of scale:1 at oracle_vad/speaker_outputs/embeddings/subsegments_scale1_embeddings.pkl\n",
      "[NeMo I 2025-05-14 21:35:43 msdd_models:988] Loading embedding pickle file of scale:2 at oracle_vad/speaker_outputs/embeddings/subsegments_scale2_embeddings.pkl\n",
      "[NeMo I 2025-05-14 21:35:43 msdd_models:988] Loading embedding pickle file of scale:3 at oracle_vad/speaker_outputs/embeddings/subsegments_scale3_embeddings.pkl\n",
      "[NeMo I 2025-05-14 21:35:43 msdd_models:988] Loading embedding pickle file of scale:4 at oracle_vad/speaker_outputs/embeddings/subsegments_scale4_embeddings.pkl\n",
      "[NeMo I 2025-05-14 21:35:43 msdd_models:966] Loading cluster label file from oracle_vad/speaker_outputs/subsegments_scale4_cluster.label\n",
      "[NeMo I 2025-05-14 21:35:43 collections:1212] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2025-05-14 21:35:43 collections:1216] Total 1 session files loaded accounting to # 1 audio clips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:44 msdd_models:1444]      [Threshold: 0.7000] [use_clus_as_main=False] [diar_window=50]\n",
      "[NeMo I 2025-05-14 21:35:44 speaker_utils:92] Number of files to diarize: 1\n",
      "[NeMo I 2025-05-14 21:35:44 speaker_utils:92] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2025-05-14 21:35:44 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:44 speaker_utils:92] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-14 21:35:44 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:44 speaker_utils:92] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-14 21:35:44 der:217] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-14 21:35:44 msdd_models:1473]   \n",
      "    \n",
      "Elapsed time: 1432.97 seconds\n"
     ]
    }
   ],
   "source": [
    "process_single_video(\"./Data/FtF32A.MP4\", \"Test\", prob_threshold=0.9, method=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59babcb2-0391-47ca-85a2-ecaa6a71a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_video(\"NATA02R.mp4\", \"Test\", prob_threshold=0.9, method=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168bb12-9749-462a-880e-54caf4965f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"/mnt/d/RICKY NEW DECEPTION\"\n",
    "\n",
    "output_directory = \"Transcripts/NEW DECEPTION\"\n",
    "\n",
    "run_all_folders(input_directory, output_directory, folders=[\"NATA##\", \"NFTF##\", \"NVTV##\"], methods=[\"dyad\", \"dyad\", \"dyad\"], threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d1982-f43c-44af-a61d-f6c7d9986d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"/mnt/d/RICKY NEW TRUST\"\n",
    "\n",
    "output_directory = \"Transcripts/NEW TRUST\"\n",
    "\n",
    "run_all_folders(input_directory, output_directory, folders=[\"FTF##\", \"VTV##\"], methods=[\"dyad\", \"dyad\"], threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7755e0-dcaf-4379-8de3-55527fb7c575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transcript)",
   "language": "python",
   "name": "transcript"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
