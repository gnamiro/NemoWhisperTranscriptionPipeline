{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca777e8b-4bd5-454e-8c4c-5f85823f3566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-12 15:31:47 transformer_bpe_models:59] Could not import NeMo NLP collection which is required for speech translation model.\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import numpy as np\n",
    "import wget\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import whisper\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.asr.models import ClusteringDiarizer, NeuralDiarizer\n",
    "import os\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b21019-26ab-42dd-b1d2-8548561e538f",
   "metadata": {},
   "source": [
    "FFmpeg is used to extract the audio from the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7541300f-5a77-460d-88dd-6f4bc27ae4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(input_video, output_audio, start_time='00:00:00', duration='00:15:00'):\n",
    "    # Check if output file exists and remove it\n",
    "    if os.path.exists(output_audio):\n",
    "        os.remove(output_audio)\n",
    "\n",
    "    # Use ffmpeg to extract the audio\n",
    "    try:\n",
    "        ffmpeg.input(input_video, ss=start_time, t=duration).output(output_audio, qscale=0, ar=16000, ac=1).run(overwrite_output=True, capture_stdout=True)\n",
    "    except ffmpeg.Error as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3013e6-33f3-43d7-a480-49a66f05cb95",
   "metadata": {},
   "source": [
    "This function counts the instances of a list of words in the transcription lines. This is used to find the phrase \"8 minute conversation\", which is used to remove the beginning of the transcription that doesn't involve the participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec652b28-c94a-4e7b-952b-5261bdf4afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(row, words):\n",
    "    return sum(word in row for word in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de494d0-0cb9-49c7-9cc8-56adda2433f9",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is used to transcribe the audio into text, the transcribe_section function takes a segment of audio and transcribes the segment into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bcb573-f5b6-41cc-9f6e-e8b39b3df31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-12 15:32:00 nemo_logging:349] C:\\Users\\Portul\\.conda\\envs\\nemo\\lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      checkpoint = torch.load(fp, map_location=device)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "model = whisper.load_model(\"base.en\")  # Load the Whisper model in English with the \"base.en\" configuration\n",
    "\n",
    "# Function to extract and transcribe a section of audio\n",
    "def transcribe_section(audio, start_time, end_time, threshold):\n",
    "    # Extract the section from the audio based on start and end times\n",
    "    section = audio[start_time*1000:end_time*1000]  # Convert times from seconds to milliseconds\n",
    "    \n",
    "    # Export the audio section to a temporary file\n",
    "    temp_file = \"temp_section.wav\"  # Temporary file name\n",
    "    section.export(temp_file, format=\"wav\")  # Save the section as a .wav file for transcription\n",
    "    \n",
    "    try:\n",
    "        # Transcribe the temporary file using Whisper\n",
    "        result = model.transcribe(temp_file)\n",
    "        \n",
    "        # Initialize an empty string to hold the filtered transcription\n",
    "        filtered_text = \"\"\n",
    "        \n",
    "        # If there are segments in the transcription result, process each segment\n",
    "        if len(result[\"segments\"]) > 0:\n",
    "            for segment in result[\"segments\"]:\n",
    "                # Only include text from segments with a no-speech probability below the threshold\n",
    "                if segment[\"no_speech_prob\"] < threshold:\n",
    "                    filtered_text += segment[\"text\"]  # Append the transcribed text to the output\n",
    "                \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Handle errors from subprocess (e.g., issues with audio extraction)\n",
    "        return ''\n",
    "    except RuntimeError as e:\n",
    "        # Print a runtime error message and return an empty string if a RuntimeError occurs\n",
    "        print(f\"RuntimeError: Error processing section from {start_time} to {end_time}: {e}\")\n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        # Print a general error message for any other exceptions\n",
    "        print(f\"Unexpected error: Error processing section from {start_time} to {end_time}: {e}\")\n",
    "        return ''\n",
    "\n",
    "    return filtered_text  # Return the filtered transcription text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80c050-5401-4571-8be4-b23e374a6a4c",
   "metadata": {},
   "source": [
    "We only want to process mp4 files, so this function checks to see if the files are mp4 files, or start with \"._\", which are metadata files generated on macOS which we don't want to process.\n",
    "\n",
    "The single parameter is used to select a single file for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4471329-21a1-4cf2-8523-70ddd7fb20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file should be processed\n",
    "def should_process_file(file_path, existing_files, single):\n",
    "    file_name = file_path.stem\n",
    "    if file_name in existing_files or \\\n",
    "       (file_path.suffix not in ['.mp4', '.MP4']) or \\\n",
    "       file_name.startswith('._') or \\\n",
    "       (single is not None and file_name not in single):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabfd71-9595-41ff-ac84-b94cabe293e9",
   "metadata": {},
   "source": [
    "This section uses NVIDIA's NeMo diarization model to diarize the extracted audio. Diarization is the process of identifying different speakers in audio, which is necessary to produce accurate transcriptions. The hyperparameters were chosen by following the guide found here: https://github.com/NVIDIA/NeMo/blob/main/tutorials/speaker_tasks/Speaker_Diarization_Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7228b1c-465e-4f8f-9a21-c8a95de56973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diarize the audio to separate speakers\n",
    "def diarize_audio(output_audio, model_config_url=\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_telephonic.yaml\"):\n",
    "    # Prepare metadata for NeMo diarization\n",
    "    meta = {'audio_filepath': output_audio, 'offset': 0, 'duration': None, 'label': 'infer', 'text': '-', 'num_speakers': None, 'rttm_filepath': None, 'uem_filepath': None}\n",
    "    with open(\"input_manifest.json\", 'w', encoding='utf-8') as fp:\n",
    "        json.dump(meta, fp)\n",
    "        fp.write('\\n')\n",
    "\n",
    "    # Load or download diarizer configuration\n",
    "    model_config_path = \"model_data/diar_infer_telephonic.yaml\"\n",
    "    if not os.path.exists(model_config_path):\n",
    "        os.makedirs(\"model_data\", exist_ok=True)\n",
    "        model_config_path = wget.download(model_config_url, \"model_data\")\n",
    "    \n",
    "    # Set configuration parameters\n",
    "    config = OmegaConf.load(model_config_path)\n",
    "    config.diarizer.manifest_filepath = \"input_manifest.json\"\n",
    "    config.diarizer.out_dir = \"oracle_vad\"\n",
    "    config.device = \"cuda\"\n",
    "    config.num_workers = 0\n",
    "    \n",
    "    # Initialize and run the diarization model\n",
    "    diarizer_model = NeuralDiarizer(cfg=config)\n",
    "    diarizer_model.diarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec4030-fa64-453e-a69b-83b480c9ac37",
   "metadata": {},
   "source": [
    "The output of the diarization is a rttm file which we use to extract the audio segments attributed to each speaker.\n",
    "\n",
    "The threshold paremeter is used to ignore sections that are shorter than the threshold. The default value is 0.5, which ignores all audio sections that are shorter than 0.5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43985325-d911-488f-b882-db7001d0af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and aggregate speaker segments\n",
    "def get_speaker_segments(rttm_path, threshold=0.5):\n",
    "    columns = ['type', 'file_id', 'channel_id', 'begin_time', 'duration', 'ortho', 'speaker_type', 'speaker_name', 'confidence_score', 'signal_lookahead']\n",
    "    rttm_df = pd.read_csv(rttm_path, delim_whitespace=True, names=columns, comment='#')\n",
    "\n",
    "    audio_sections = []\n",
    "    current_section = None\n",
    "    for _, row in rttm_df.iterrows():\n",
    "        begin_time, end_time, speaker = row[\"begin_time\"], row[\"begin_time\"] + row[\"duration\"], row[\"speaker_name\"]\n",
    "        if row[\"duration\"] > threshold:\n",
    "            if current_section is None or current_section[2] != speaker or begin_time - current_section[1] >= 1:\n",
    "                if current_section:\n",
    "                    audio_sections.append(current_section)\n",
    "                current_section = [begin_time, end_time, speaker]\n",
    "            else:\n",
    "                current_section[1] = end_time\n",
    "    if current_section:\n",
    "        audio_sections.append(current_section)\n",
    "\n",
    "    return audio_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840890b-6dca-4c26-af53-00138a6e35ef",
   "metadata": {},
   "source": [
    "With the extracted segments, we can the transcribe each section using Whisper.\n",
    "\n",
    "Whisper outputs a value that represents the percent chance the section isn't speech. The threshold parameter ignores sections where this chance is above the threshold. By default it is set to 0.9, which ignores all segements with a 90% or higher chance to not be speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39ba98e-49b9-498c-a149-563d89689307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe each speaker section\n",
    "def transcribe_sections(audio, audio_sections, threshold=0.9):\n",
    "    transcriptions = []\n",
    "    first_speech = audio_sections[0][0] if audio_sections else 0\n",
    "\n",
    "    for start_time, end_time, speaker in audio_sections:\n",
    "        text = transcribe_section(audio, start_time, end_time, threshold)\n",
    "        if text:\n",
    "            transcriptions.append([start_time - first_speech, end_time - first_speech, speaker, text])\n",
    "    return pd.DataFrame(transcriptions, columns=[\"Start Time\", \"End Time\", \"Speaker\", \"Transcription\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285a574-1c19-4b6a-b836-cef2a9c81011",
   "metadata": {},
   "source": [
    "Once the transcriptions are complete, we remove the noise at the beginning of the transcription and identify the 2 most prominent speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1738bfc1-4be6-447d-9864-4a86c84accad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and label speakers in transcriptions\n",
    "def filter_and_label_speakers(df):\n",
    "    df['match_count'] = df['Transcription'].apply(lambda row: count_matches(row, [\"8\", \"eight\", \"minute\", \"conversation\", \"chat\"]))\n",
    "    index_RA = df.index.get_loc(df['match_count'].idxmax())\n",
    "    filtered_df = df.iloc[index_RA + 1:][[\"Start Time\", \"End Time\", \"Speaker\", \"Transcription\"]] if df.iloc[index_RA][\"Start Time\"] / df.iloc[-1][\"Start Time\"] < 0.5 and df['match_count'].max() > 1 else df\n",
    "\n",
    "    top_2_speakers = filtered_df['Speaker'].value_counts().nlargest(2).index.tolist()\n",
    "    replace_map = {top_2_speakers[0]: \"Speaker 1\", top_2_speakers[1]: \"Speaker 2\"} if len(top_2_speakers) > 1 else {top_2_speakers[0]: \"Speaker 1\"}\n",
    "    return filtered_df.replace({\"Speaker\": replace_map})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724383a3-60f7-48fd-a296-95163041c388",
   "metadata": {},
   "source": [
    "We then save the transcriptions after splitting the 2 most prominent speakers, which allow us to produce both the \"dyad\" files which include both speakers as well as \"single\" files which include only 1 speaker. We also include a \"full\" file which are the unfiltered transcriptions, this allows us to regenerate the \"dyad\" and \"single\" files without having to rerun the entire pipeline.\n",
    "\n",
    "The method paramter selects whether the pipeline should treat the input file as having 2 speakers in the case method == 0, or only 1 speaker in the case method != 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435469e5-a944-4d61-8690-9b810b93b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transcription results based on method\n",
    "def save_transcriptions(df, output_directory, file_name, method):\n",
    "    if method == 0:\n",
    "        df[(df[\"Speaker\"] == \"Speaker 1\") | (df[\"Speaker\"] == \"Speaker 2\")].to_csv(output_directory + \"/dyad/\" + file_name + '_dyad.txt', sep=\"|\", index=False)\n",
    "        df[df[\"Speaker\"] == \"Speaker 1\"].to_csv(output_directory + \"/single/\" + file_name + '_single_X.txt', sep=\"|\", index=False)\n",
    "        df[df[\"Speaker\"] == \"Speaker 2\"].to_csv(output_directory + \"/single/\" + file_name + '_single_Y.txt', sep=\"|\", index=False)\n",
    "    else:\n",
    "        df[df[\"Speaker\"] == \"Speaker 1\"].to_csv(output_directory + \"/single/\" + file_name + '_single.txt', sep=\"|\", index=False)\n",
    "    df.to_csv(output_directory + \"/full/\" + file_name + '_full.txt', sep=\"|\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee886c19-1808-49e8-bf0e-5d07afb16839",
   "metadata": {},
   "source": [
    "This function runs the full pipeline on all files in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cfd3f9-1bbb-46bd-aace-d30126b4e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process videos in a folder\n",
    "def run_in_folder(input_directory, output_directory, single=None, threshold=0.9, method=None):\n",
    "    directory_path = Path(input_directory)\n",
    "    output_directory_path = Path(output_directory)\n",
    "    existing_files = [file.stem.split(\"_\")[0] for sub_folder in [\"dyad\", \"full\", \"single\"] for file in (output_directory_path / sub_folder).glob(\"*\")]\n",
    "\n",
    "    for file_path in directory_path.glob(\"*.mp4\"):\n",
    "        if should_process_file(file_path, existing_files, single):\n",
    "            print(f\"Processing file: {file_path.stem}\")\n",
    "            workflow_start_time = time.time()\n",
    "            \n",
    "            # Step 1: Extract Audio\n",
    "            output_audio = 'temp_extracted_audio.wav'\n",
    "            extract_audio(file_path, output_audio)\n",
    "\n",
    "            # Step 2: Diarize\n",
    "            diarize_audio(output_audio)\n",
    "            \n",
    "            # Step 3: Get Speaker Segments\n",
    "            audio_sections = get_speaker_segments('oracle_vad/pred_rttms/temp_extracted_audio.rttm')\n",
    "            \n",
    "            # Step 4: Transcribe Sections\n",
    "            audio = AudioSegment.from_wav(output_audio)\n",
    "            df = transcribe_sections(audio, audio_sections, threshold)\n",
    "            \n",
    "            # Step 5: Filter and Label Speakers\n",
    "            filtered_df = filter_and_label_speakers(df)\n",
    "            \n",
    "            # Step 6: Save Transcriptions\n",
    "            save_transcriptions(filtered_df, output_directory, file_path.stem, method)\n",
    "            \n",
    "            # Clean up temporary audio file\n",
    "            os.remove(output_audio) if os.path.exists(output_audio) else None\n",
    "            print(f\"Elapsed time: {round(time.time() - workflow_start_time, 2)} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e3ae9-6e0d-46f7-891a-c2842522c2b2",
   "metadata": {},
   "source": [
    "This function loops through a list of subfolders to execute the main pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a7c79e-9de6-4283-bd67-59cd6df07678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple folders\n",
    "def run_all_folders(input_directory, output_directory, folders=[\"VTV\", \"FTF\", \"VGC\"], methods=[None, None, None], single=None, threshold=0.9):\n",
    "    for folder, method in zip(folders, methods):\n",
    "        run_in_folder(input_directory + \"/\" + folder + \"/\", output_directory + \"/\" + folder + \"/\", single=single, threshold=threshold, method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df8bde-07ec-4ba9-8105-8dc6f5d3b640",
   "metadata": {},
   "source": [
    "Here is an example execution of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ea895-f4f0-4d16-8d1c-e5c9b31d53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"K:/Study 2 (Fall2023)\"\n",
    "\n",
    "output_directory = \"Transcripts/Study 2 (Fall2023)\"\n",
    "\n",
    "run_all_folders(input_directory, output_directory, folders=[\"VTV\", \"FTF\", \"VGC\"], methods=[1, 0,0], single=None, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168bb12-9749-462a-880e-54caf4965f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
