{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fd861d4-3198-4bae-9baa-3b5e189f1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e05e3c-329e-4090-bbc7-d31c41aabcec",
   "metadata": {},
   "source": [
    "This section reads 2 VtV files and prepares them to be merged. method == 0 reads files that end with _single.txt, these are the single speaker transcriptions produced from videos where there is only 1 speaker. method != 0 reads files that end with _single_X.txt and _single_Y.txt, these are single speaker transcriptions produced by splitting the speakers from videos with 2 speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84667512-4ea5-4b3e-b752-e1f73ce3835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 0\n",
    "if method == 0: # Merge seperate transcriptions\n",
    "    directory = \"Transcripts/Study 2 (Fall2023)/VTV/\"\n",
    "    file_start = \"VtV36\"\n",
    "    \n",
    "    dfA = pd.read_csv(directory + \"single/\"+ file_start + \"A_single.txt\", sep=\"|\")\n",
    "    dfB = pd.read_csv(directory + \"single/\"+ file_start + \"B_single.txt\", sep=\"|\")\n",
    "else: # Merge pre split transcriptions\n",
    "    directory = \"Transcripts/Study 2 (Fall2023)/VTV/\"\n",
    "    file_start = \"VtV17\"\n",
    "    \n",
    "    dfA = pd.read_csv(directory + \"single/\"+ file_start + \"A_single_X.txt\", sep=\"|\")\n",
    "    dfB = pd.read_csv(directory + \"single/\"+ file_start + \"B_single_Y.txt\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fc24b-cdd8-454a-8e05-b1d1b003b0cd",
   "metadata": {},
   "source": [
    "To merge, we select 2 timestamps in both transcriptions that have been manually determined to be identical in actual time. We then calculate the difference and subtract the difference from the first transcriptions file, which synchronizes the timestamps for both files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f10629ff-b6c5-4e89-8450-cf34ba325fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA_time = 123.28999999999999\n",
    "dfB_time = 122.24000000000001\n",
    "difference = dfA_time - dfB_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd4e38e9-b598-4c75-ad26-84bb83b53643",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA[\"Start Time\"] = dfA[\"Start Time\"] -difference\n",
    "dfA[\"End Time\"]= dfA[\"End Time\"] -difference\n",
    "dfA[\"Speaker\"] = \"Speaker 1\"\n",
    "dfB[\"Speaker\"] = \"Speaker 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a868b7-febd-4267-950c-6d1ab8bbc6ec",
   "metadata": {},
   "source": [
    "We can then merge both files and then sort by the start time of each line of speech. This produces a merged transcription that we can now save to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcb006ec-3ee3-411e-a8fc-e00fb20fdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([dfA, dfB])\n",
    "df_new = df_new.sort_values(by=[\"Start Time\"])\n",
    "df_new.to_csv(directory + \"dyad/\"+ file_start + \"_dyad.txt\", index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c2f87-4d2e-4495-bc37-c3f75c80ecdc",
   "metadata": {},
   "source": [
    "# Merge videos to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022f063d-f265-4425-a745-642e2f9a893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def combine_dual_recordings(\n",
    "    file_a,\n",
    "    file_b,\n",
    "    output_path=\"combined.wav\",\n",
    "    align_offset_ms=0,\n",
    "    stereo=True,\n",
    "    target_sample_rate=16000\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines two mono recordings (e.g., from two speakers) into a single audio file.\n",
    "    \n",
    "    Parameters:\n",
    "        file_a (str): Path to speaker A's audio file\n",
    "        file_b (str): Path to speaker B's audio file\n",
    "        output_path (str): Output WAV path for the combined audio\n",
    "        align_offset_ms (int): Offset in milliseconds to delay speaker B (can be negative)\n",
    "        stereo (bool): If True, saves as stereo (speaker A = left, B = right). If False, mixes into mono\n",
    "        target_sample_rate (int): Output sample rate (Hz), default is 16000\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Loading audio files: {file_a} and {file_b}\")\n",
    "    a = AudioSegment.from_file(file_a).set_frame_rate(target_sample_rate).set_channels(1)\n",
    "    b = AudioSegment.from_file(file_b).set_frame_rate(target_sample_rate).set_channels(1)\n",
    "\n",
    "    # Apply the offset\n",
    "    if align_offset_ms > 0:\n",
    "        print(f\"‚è±Ô∏è Delaying speaker B by {align_offset_ms} ms\")\n",
    "        b = AudioSegment.silent(duration=align_offset_ms) + b\n",
    "    elif align_offset_ms < 0:\n",
    "        print(f\"‚è±Ô∏è Delaying speaker A by {-align_offset_ms} ms\")\n",
    "        a = AudioSegment.silent(duration=-align_offset_ms) + a\n",
    "\n",
    "    # Equalize lengths by padding\n",
    "    max_len = max(len(a), len(b))\n",
    "    a += AudioSegment.silent(duration=max_len - len(a))\n",
    "    b += AudioSegment.silent(duration=max_len - len(b))\n",
    "\n",
    "    # Combine (stereo option)\n",
    "    if stereo:\n",
    "        print(\"üîä Combining as stereo (A = left, B = right)\")\n",
    "        combined = AudioSegment.from_mono_audiosegments(a, b)\n",
    "    else:\n",
    "        print(\"üîä Combining as mono overlay\")\n",
    "        combined = a.overlay(b)\n",
    "\n",
    "    # Export\n",
    "    combined.export(output_path, format=\"wav\")\n",
    "    print(f\"‚úÖ Combined audio saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c082c32-bc61-4ead-b62d-6f3d7b76fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading audio files: ./Data/DECEPTION/ATA/NATA41 (SUSPECTED).m4a and ./Data/DECEPTION/ATA/NATA41-R (UNPINNED).mp4\n",
      "üîä Combining as stereo (A = left, B = right)\n",
      "‚úÖ Combined audio saved to: combined_3_s.wav\n"
     ]
    }
   ],
   "source": [
    "study_a = \"./Data/DECEPTION/ATA/NATA41 (SUSPECTED).m4a\"\n",
    "study_b = \"./Data/DECEPTION/ATA/NATA41-R (UNPINNED).mp4\"\n",
    "combine_dual_recordings(\n",
    "    file_a=study_a,\n",
    "    file_b=study_b,\n",
    "    output_path=\"combined_3_s.wav\",\n",
    "    align_offset_ms=0,  # Set this manually if there's a delay\n",
    "    stereo=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825581c-9a93-4ca2-8014-85633b7193f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
