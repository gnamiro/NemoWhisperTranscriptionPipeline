{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7738bb22-eca9-4917-a51f-09ae2403135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Start Time      End Time    Speaker  \\\n",
      "0  00:00:00.000  00:00:01.050  speaker_2   \n",
      "1  00:00:02.220  00:00:03.450  speaker_2   \n",
      "2  00:00:04.219  00:00:04.729  Speaker 1   \n",
      "3  00:00:06.059  00:00:06.684  Speaker 1   \n",
      "4  00:00:06.684  00:00:08.250  speaker_2   \n",
      "\n",
      "                                     Transcription  \n",
      "0                       I'm going to send you the.  \n",
      "1                         I guess it is reporting.  \n",
      "2  Take two thdays and.... speak to you tatue ball  \n",
      "3                                  congratulations  \n",
      "4                   Yep, this has changed already.  \n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# First let's make sure to read the transcription correctly\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_transcription_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line for line in f if not line.startswith('#') and line.strip() != '']\n",
    "\n",
    "\n",
    "    from io import StringIO\n",
    "    data_str = ''.join(lines)\n",
    "    df = pd.read_csv(StringIO(data_str), sep='|', header=None, names=['Start Time', 'End Time', 'Speaker', 'Transcription'])\n",
    "\n",
    "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "file_path = 'Test/full/NVTV25-R (SUSPECTED)_full.txt'\n",
    "# file_path = 'Test/full/NATA41 (SUSPECTED)_full.txt'\n",
    "# file_path = 'Test/full/NVTV35 (SUSPECTED)_full.txt'\n",
    "# NVTV26 (SUSPECTED)_full\n",
    "# NVTV25-R (SUSPECTED)_full\n",
    "\n",
    "df = read_transcription_file(file_path)\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b00075-12fd-4d47-8672-3a561cd0c94c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbc0d3c9-e0e2-46f1-afd9-98b6ea13a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_speaker_label(s):\n",
    "    match = re.search(r'(\\d+)', s)\n",
    "    if match:\n",
    "        return f\"Speaker {match.group(1)}\"\n",
    "    return s.strip().title()\n",
    "\n",
    "df['Speaker'] = df['Speaker'].apply(normalize_speaker_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16e8e1c9-87d7-45e7-8bee-696003e0ee31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Speaker 0', 'Speaker 1', 'Speaker 2'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Speaker'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2d99a-1ad6-4dcd-88dd-5f86826291b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b92004b-d6ba-4895-92a0-dbe6f37da1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf6bebd-9e9a-4132-8609-47c486e26395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_dialogue(df):\n",
    "    lines = []\n",
    "    for _, row in df.iterrows():\n",
    "        speaker = row[\"Speaker\"]\n",
    "        text = row[\"Transcription\"]\n",
    "        lines.append(f\"{speaker}: {text}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4af776d-0585-485d-ab8b-3f36aa05402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_roles_with_gpt(conversation_text):\n",
    "    system_prompt = (\n",
    "        \"You are an assistant helping identify speaker roles in a two-person conversation. \"\n",
    "        \"Ali is the person offering money. Rowan is the one deciding to accept or reject.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Given the following conversation, determine who is Ali and who is Rowan.\\n\\n\"\n",
    "        f\"{conversation_text}\\n\\n\"\n",
    "        \"Please respond in the format:\\n\"\n",
    "        \"Speaker 1 is <NAME>\\n\"\n",
    "        \"Speaker 2 is <NAME>\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9984bb-af5c-45c0-aa26-520db2bb7781",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m get_sample_dialogue(df)\n\u001b[0;32m----> 2\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[43midentify_roles_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_output)\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36midentify_roles_with_gpt\u001b[0;34m(conversation_text)\u001b[0m\n\u001b[1;32m      2\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an assistant helping identify speaker roles in a two-person conversation. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAli is the person offering money. Rowan is the one deciding to accept or reject.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven the following conversation, determine who is Ali and who is Rowan.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversation_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeaker 2 is <NAME>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/resources/chat/completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transcript/lib/python3.10/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "sample_text = get_sample_dialogue(df)\n",
    "llm_output = identify_roles_with_gpt(sample_text)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7363aea-b6c3-45db-bd53-53aecab296e4",
   "metadata": {},
   "source": [
    "# Role classification via Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e2c564-6942-40a1-9263-cf0d6e122959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0488459ac1e426bb59b0e4ba301666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2920dfb8513440d2b2a60d8dc5b40028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa42984eede45989cfec8d7fee133c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794b1ffc10904175bcda619743111c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd80e1647824126a8acbd37df7e957f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee47da7b76e4896b291ea5cc8c16de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9208bbc603446e2840d575cec3eb69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a472940f7b364f1e8c1767c6f75c2ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0c935f41544f2aa27541b56d34d990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fbabc6433844c68116e65386d91820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd58701cf2c14ca5a5d879fbb022b1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Semantic base Role Classification\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "giver_prompts = [\n",
    "    \"I offer you dollars.\",\n",
    "    \"I am offering you some dollars\",\n",
    "    \"Here is the amount I'm giving.\",\n",
    "    \"I decide how much to allocate.\",\n",
    "    \"This is my offer\",\n",
    "    \"Here is my offer\",\n",
    "    \"I decide how much money to give\",\n",
    "    \"I am going to give you\",\n",
    "]\n",
    "receiver_prompts = [\n",
    "    \"I will accept your offer.\",\n",
    "    \"I am rejecting the offer.\",\n",
    "    \"I reject that\",\n",
    "    \"I need to think about your offer\",\n",
    "    \"You are offering me money?\",\n",
    "    \"I get to decide if the offer is fair.\",\n",
    "    \"I am going to accept it\",\n",
    "    \"I will reject it\",\n",
    "    \"No, I don't want it\",\n",
    "]\n",
    "\n",
    "giver_embed = torch.stack([model.encode(p, convert_to_tensor=True) for p in giver_prompts]).mean(dim=0)\n",
    "receiver_embed = torch.stack([model.encode(p, convert_to_tensor=True) for p in receiver_prompts]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e259ada-2cf7-4a92-80a0-049bf33acb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def assign_roles(df, text_col=\"Transcription\", speaker_col=\"Speaker\", top_k=50):\n",
    "    # Focus on top_k utterances for each speaker\n",
    "    role_map = {}\n",
    "    for speaker in df[speaker_col].unique():\n",
    "        texts = df[df[speaker_col] == speaker][text_col].tolist()[:top_k]\n",
    "        if not texts:\n",
    "            continue\n",
    "        speaker_embs = model.encode(texts, convert_to_tensor=True)\n",
    "        # Compute similarity\n",
    "        giver_score = util.cos_sim(speaker_embs, giver_embed).mean().item()\n",
    "        receiver_score = util.cos_sim(speaker_embs, receiver_embed).mean().item()\n",
    "        role_map[speaker] = \"Giver\" if giver_score > receiver_score else \"Receiver\"\n",
    "        print(f\"speaker: {speaker} --> scores: {giver_score}, {receiver_score}\")\n",
    "    return role_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10a86329-f188-4540-9df1-00d92a4bbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_roles_by_embedding(df):\n",
    "    # Group all text per speaker\n",
    "    speaker_texts = {\n",
    "        speaker: \" \".join(df[df[\"Speaker\"] == speaker][\"Transcription\"].tolist())\n",
    "        for speaker in df[\"Speaker\"].unique()\n",
    "    }\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for speaker, text in speaker_texts.items():\n",
    "        if not text.strip():\n",
    "            scores[speaker] = {\"giver\": 0.0, \"receiver\": 0.0}\n",
    "            continue\n",
    "\n",
    "        text_embed = model.encode(text, convert_to_tensor=True)\n",
    "        scores[speaker] = {\n",
    "            \"giver\": util.cos_sim(text_embed, giver_embed).item(),\n",
    "            \"receiver\": util.cos_sim(text_embed, receiver_embed).item()\n",
    "        }\n",
    "\n",
    "    # Compute score difference\n",
    "    role_scores = {\n",
    "        speaker: score[\"giver\"] - score[\"receiver\"]\n",
    "        for speaker, score in scores.items()\n",
    "    }\n",
    "\n",
    "    sorted_speakers = sorted(role_scores, key=role_scores.get, reverse=True)\n",
    "    print(role_scores)\n",
    "    if len(sorted_speakers) >= 2:\n",
    "        return {\n",
    "            sorted_speakers[0]: \"Giver\",\n",
    "            sorted_speakers[-1]: \"Receiver\"\n",
    "        }\n",
    "    else:\n",
    "        # Fallback (only one speaker)\n",
    "        return {sorted_speakers[0]: \"Giver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef67636c-c843-40a5-9c16-058a529dfff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker: Speaker 2 --> scores: 0.76962810754776, 0.7655006051063538\n",
      "speaker: Speaker 1 --> scores: 0.7702497839927673, 0.776236891746521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Speaker 2': 'Giver', 'Speaker 1': 'Receiver'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_map = assign_roles(df)\n",
    "role_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37bbbe84-a3de-4ffd-a03b-3872725b35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Speaker 2': -0.07934367656707764, 'Speaker 1': 0.0026491880416870117}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Speaker 1': 'Giver', 'Speaker 2': 'Receiver'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NVTV25\n",
    "determine_roles_by_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fad973f8-ef9d-422a-bc1d-9571b66130a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Speaker 0': -0.03853234648704529, 'Speaker 1': 0.01988561451435089, 'Speaker 2': 0.0289078950881958}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Speaker 2': 'Giver', 'Speaker 0': 'Receiver'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NATA41\n",
    "determine_roles_by_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f2e76f2-0618-41d9-b0b1-f9860864d10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Speaker 1': 0.06889933347702026, 'speaker_4': 0.067948117852211, 'Speaker 2': 0.0006491243839263916, 'speaker_2': 0.03777685761451721, 'speaker_3': 0.06741087883710861}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Speaker 1': 'Giver', 'Speaker 2': 'Receiver'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NVTV35\n",
    "determine_roles_by_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87520ab-2af8-42e6-9542-4cc17e171161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Speaker 1': 0.06889933347702026, 'Speaker 4': 0.067948117852211, 'Speaker 2': 0.024840623140335083, 'Speaker 3': 0.06741087883710861}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Speaker 1': 'Giver', 'Speaker 2': 'Receiver'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NVTV35\n",
    "determine_roles_by_embedding(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f36d4-f328-4da2-a28e-26c2b91a1fca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TF-IDF+ LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ebc5cb3-8dd1-489c-ae8f-fa2ffb21717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Giver       0.50      1.00      0.67         1\n",
      "    Receiver       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "I'm offering you ten dollars. --> Giver\n",
      "I think this is a fair deal. --> Giver\n",
      "I'm not sure about your offer. --> Receiver\n",
      "Yes, I accept your offer. --> Receiver\n",
      "Why would you offer that? --> Receiver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gnamiro/miniconda3/envs/transcript/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/gnamiro/miniconda3/envs/transcript/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/gnamiro/miniconda3/envs/transcript/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example labeled training data\n",
    "data = [\n",
    "    (\"I offer you five dollars.\", \"Giver\"),\n",
    "    (\"Here is the amount I'm giving.\", \"Giver\"),\n",
    "    (\"You can take this money.\", \"Giver\"),\n",
    "    (\"I think five is fair.\", \"Giver\"),\n",
    "    (\"I will accept your offer.\", \"Receiver\"),\n",
    "    (\"Why are you giving me money?\", \"Receiver\"),\n",
    "    (\"I reject that offer.\", \"Receiver\"),\n",
    "    (\"I don't think it's enough.\", \"Receiver\"),\n",
    "    # Optionally add neutral/other roles with \"Other\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"utterance\", \"role\"])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"utterance\"], df[\"role\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define pipeline: TF-IDF + Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2))),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on new utterances\n",
    "new_utterances = [\n",
    "    \"I'm offering you ten dollars.\",\n",
    "    \"I think this is a fair deal.\",\n",
    "    \"I'm not sure about your offer.\",\n",
    "    \"Yes, I accept your offer.\",\n",
    "    \"Why would you offer that?\"\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(new_utterances)\n",
    "\n",
    "for utterance, role in zip(new_utterances, predictions):\n",
    "    print(f\"{utterance} --> {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818310f3-7f7c-41a1-974b-640f7181a2c4",
   "metadata": {},
   "source": [
    "## Keyword matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9983940-41df-4db0-b540-be5aad0d2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def preprocess(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Phrases and keywords\n",
    "giver_keywords = {\"offer\", \"give\", \"giving\", \"allocate\", \"amount\", \"money\", \"dollars\"}\n",
    "receiver_keywords = {\"accept\", \"reject\", \"take\", \"fair\", \"decline\", \"deal\"}\n",
    "\n",
    "giver_phrases = [\"i'm offering\", \"i offer\", \"i'm supposed to offer\", \"i decide how much\"]\n",
    "receiver_phrases = [\"i accept\", \"i reject\", \"do you accept\", \"i'll accept\", \"they're offering me\"]\n",
    "\n",
    "# Observer indicators\n",
    "observer_keywords = [\"end the call\", \"have a couple surveys\", \"see you\", \"i'll just end\"]\n",
    "\n",
    "def identify_roles_with_observer_check(df):\n",
    "    speaker_scores = defaultdict(lambda: {\"giver\": 0, \"receiver\": 0, \"is_observer\": False})\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        speaker = row[\"Speaker\"].strip().lower()\n",
    "        text = row[\"Transcription\"].lower()\n",
    "\n",
    "        if any(obs in text for obs in observer_keywords):\n",
    "            speaker_scores[speaker][\"is_observer\"] = True\n",
    "            continue\n",
    "\n",
    "        # Phrase-based boosting\n",
    "        for phrase in giver_phrases:\n",
    "            if phrase in text:\n",
    "                speaker_scores[speaker][\"giver\"] += 3\n",
    "        for phrase in receiver_phrases:\n",
    "            if phrase in text:\n",
    "                speaker_scores[speaker][\"receiver\"] += 3\n",
    "\n",
    "        # Token-based keyword scoring\n",
    "        tokens = preprocess(text)\n",
    "        for token in tokens:\n",
    "            if token in giver_keywords:\n",
    "                speaker_scores[speaker][\"giver\"] += 1\n",
    "            if token in receiver_keywords:\n",
    "                speaker_scores[speaker][\"receiver\"] += 1\n",
    "\n",
    "    # Assign final roles\n",
    "    roles = {}\n",
    "    for speaker, scores in speaker_scores.items():\n",
    "        if scores[\"is_observer\"]:\n",
    "            roles[speaker] = \"Observer\"\n",
    "        elif scores[\"giver\"] > scores[\"receiver\"]:\n",
    "            roles[speaker] = \"Giver\"\n",
    "        elif scores[\"receiver\"] > scores[\"giver\"]:\n",
    "            roles[speaker] = \"Receiver\"\n",
    "        else:\n",
    "            roles[speaker] = \"Unknown\"\n",
    "    return roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d61edfff-4b10-476b-a160-8ef10416d1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speaker 1': 'Receiver', 'speaker 2': 'Receiver'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NVTV25\n",
    "# print(df.head())\n",
    "identify_roles_with_observer_check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3e27499-bc41-4310-8f66-b0182f626bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speaker 0': 'Observer', 'speaker 1': 'Receiver', 'speaker 2': 'Giver'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NATA41\n",
    "# print(df.head())\n",
    "identify_roles_with_observer_check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd174a56-f902-42ce-8d9b-162c353725cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NATA35\n",
    "# print(df.head())\n",
    "identify_roles_with_observer_check(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b6ef3-5b89-4aaf-b0ee-12876ca920ad",
   "metadata": {},
   "source": [
    "# ConLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edcd428d-6ffc-4996-a5c2-084ca1e48eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Define a custom whitelist of expected names\n",
    "VALID_NAMES = {'ali', 'rowan', 'rohan', 'sam', 'bailey'}\n",
    "\n",
    "def clean_name(name):\n",
    "    name = name.strip().lower()\n",
    "    # Reject common verbs or non-names\n",
    "    if name in VALID_NAMES:\n",
    "        return name.capitalize()\n",
    "    return None\n",
    "\n",
    "def parse_transcription(transcript_text):\n",
    "    speaker_lines = []\n",
    "    for line in transcript_text.strip().split('\\n'):\n",
    "        if '|' not in line or line.startswith('#'):\n",
    "            continue\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) != 4:\n",
    "            continue\n",
    "        _, _, speaker, utterance = parts\n",
    "        speaker = speaker.strip().lower()\n",
    "        utterance = utterance.strip()\n",
    "        speaker_lines.append((speaker, utterance))\n",
    "    return speaker_lines\n",
    "\n",
    "def extract_names_with_whitelist(speaker_lines):\n",
    "    patterns = [\n",
    "        r\"\\bmy name is (\\w+)\",\n",
    "        r\"\\bi am (\\w+)\",\n",
    "        r\"\\bi'm (\\w+)\",\n",
    "        r\"\\bthis is (\\w+)\",\n",
    "        r\"\\b(\\w+) is my name\",\n",
    "    ]\n",
    "\n",
    "    candidates = defaultdict(list)\n",
    "\n",
    "    for speaker, utt in speaker_lines:\n",
    "        utt_lower = utt.lower()\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, utt_lower)\n",
    "            if match:\n",
    "                raw_name = match.group(1)\n",
    "                name = clean_name(raw_name)\n",
    "                if name:\n",
    "                    candidates[speaker].append(name)\n",
    "                break\n",
    "\n",
    "    final_names = {}\n",
    "    for speaker, name_list in candidates.items():\n",
    "        if name_list:\n",
    "            final_names[speaker] = Counter(name_list).most_common(1)[0][0]\n",
    "    return final_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e35b1e9-b3ef-4e94-9fb5-6714f930b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected speaker names:\n",
      "speaker 1: Ali\n",
      "speaker 2: Rohan\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NVTV25-R (SUSPECTED)_full.txt'\n",
    "with open(file_path, \"r\") as f:  # Or directly paste the content as a string\n",
    "        transcription = f.read()\n",
    "\n",
    "speaker_lines = parse_transcription(transcription)\n",
    "detected_names = extract_names_with_whitelist(speaker_lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in detected_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165ae4b-6196-4e12-81c2-0a70b60ba70b",
   "metadata": {},
   "source": [
    "## Using NER to detect different varations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14bd51a8-e02a-43bf-b08f-289565e72064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3f07030-d080-4b8b-81f0-eafd95ec701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from thefuzz import fuzz\n",
    "\n",
    "VALID_NAMES = ['ali', 'rowan', 'sam', 'bailey']\n",
    "BAD_NAME_TOKENS = {'okay', 'good', 'sure', 'fine', 'yes', 'no', 'cool', 'hi', 'bye', 'hello', 'alright'}\n",
    "\n",
    "SELF_PATTERNS = [\n",
    "    r\"\\bmy name is (\\w+)\\b\",\n",
    "    r\"\\bi am (\\w+)\\b\",\n",
    "    r\"\\bi'm (\\w+)\\b\",\n",
    "    r\"\\bthis is (\\w+)\\b\",\n",
    "    r\"\\b(\\w+) is my name\\b\"\n",
    "]\n",
    "\n",
    "def parse_transcription(transcript_text):\n",
    "    lines = []\n",
    "    for line in transcript_text.strip().split('\\n'):\n",
    "        if line.startswith('#') or '|' not in line:\n",
    "            continue\n",
    "        parts = line.split('|')\n",
    "        if len(parts) >= 4:\n",
    "            _, _, speaker, utterance = parts[:4]\n",
    "            lines.append((speaker.strip().lower(), utterance.strip()))\n",
    "    return lines\n",
    "\n",
    "def extract_names_by_pattern(lines):\n",
    "    name_candidates = defaultdict(list)\n",
    "\n",
    "    for speaker, utt in lines:\n",
    "        utt_lower = utt.lower()\n",
    "\n",
    "        for pattern in SELF_PATTERNS:\n",
    "            match = re.search(pattern, utt_lower)\n",
    "            if match:\n",
    "                raw_name = match.group(1).strip().lower()\n",
    "\n",
    "                if len(raw_name) < 3 or raw_name in BAD_NAME_TOKENS:\n",
    "                    break\n",
    "\n",
    "                # Fuzzy token sort ratio\n",
    "                scores = [(name, fuzz.token_sort_ratio(raw_name, name)) for name in VALID_NAMES]\n",
    "                best_match, best_score = max(scores, key=lambda x: x[1])\n",
    "                print(f\"raw name: {raw_name}, best mathc: {best_match}, score: {best_score}\")\n",
    "                if best_score >= 60:  # lowered threshold for rare names like Rola  Rowan\n",
    "                    name_candidates[speaker].append(best_match.capitalize())\n",
    "                break\n",
    "\n",
    "    speaker_names = {}\n",
    "    for speaker, names in name_candidates.items():\n",
    "        if names:\n",
    "            speaker_names[speaker] = Counter(names).most_common(1)[0][0]\n",
    "    return speaker_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6de18907-dbbe-472f-bd44-60e3c2d3f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected speaker names:\n",
      "speaker 1: Ali\n",
      "speaker 2: Rowan\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NVTV25-R (SUSPECTED)_full.txt'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "parsed_lines = parse_transcription(transcript)\n",
    "speaker_names = extract_names_by_pattern(parsed_lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in speaker_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4173a682-9471-4a0a-bbba-c4aa80badd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw name: just, best mathc: sam, score: 29\n",
      "raw name: just, best mathc: sam, score: 29\n",
      "raw name: going, best mathc: rowan, score: 40\n",
      "raw name: just, best mathc: sam, score: 29\n",
      "raw name: rola, best mathc: rowan, score: 67\n",
      "raw name: ali, best mathc: ali, score: 100\n",
      "raw name: studying, best mathc: ali, score: 18\n",
      "raw name: liking, best mathc: ali, score: 44\n",
      "raw name: currently, best mathc: rowan, score: 29\n",
      "raw name: more, best mathc: sam, score: 29\n",
      "raw name: not, best mathc: rowan, score: 25\n",
      "raw name: supposed, best mathc: sam, score: 18\n",
      "raw name: offering, best mathc: rowan, score: 31\n",
      "raw name: gonna, best mathc: rowan, score: 40\n",
      "raw name: kind, best mathc: ali, score: 29\n",
      "Detected speaker names:\n",
      "speaker 1: Rowan\n",
      "speaker 2: Ali\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NATA41-R (UNPINNED)_full.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "parsed_lines = parse_transcription(transcript)\n",
    "speaker_names = extract_names_by_pattern(parsed_lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in speaker_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b708081f-e6b4-40f9-8d0d-15a97a034e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from thefuzz import fuzz\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Known valid participant names\n",
    "VALID_NAMES = ['ali', 'rowan', 'sam', 'bailey']\n",
    "\n",
    "# Regex patterns for self-identification\n",
    "SELF_PATTERNS = [\n",
    "    r\"\\bmy name is (\\w+)\",\n",
    "    r\"\\bi am (\\w+)\",\n",
    "    r\"\\bi'm (\\w+)\",\n",
    "    r\"\\bthis is (\\w+)\",\n",
    "    r\"\\b(\\w+) is my name\",\n",
    "]\n",
    "\n",
    "INVALID_NAME_TOKENS = {\n",
    "    # Verbs (base and present participle forms)\n",
    "    \"offering\", \"offer\", \"giving\", \"give\", \"rejecting\", \"reject\", \"accepting\", \"accept\",\n",
    "    \"doing\", \"going\", \"thinking\", \"think\", \"studying\", \"study\", \"saying\", \"say\",\n",
    "    \"living\", \"live\", \"coming\", \"come\", \"taking\", \"take\", \"asking\", \"ask\",\n",
    "    \"speaking\", \"speak\", \"sitting\", \"sit\", \"watching\", \"watch\", \"talking\", \"talk\",\n",
    "    \"waiting\", \"wait\", \"starting\", \"start\", \"working\", \"work\", \"recording\", \"record\",\n",
    "\n",
    "    # Modal and auxiliary verbs\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\",\n",
    "\n",
    "    # Common adjectives/adverbs\n",
    "    \"just\", \"more\", \"most\", \"some\", \"any\", \"not\", \"even\", \"currently\", \"later\", \"now\", \"soon\", \"only\",\n",
    "\n",
    "    # Pronouns and function words\n",
    "    \"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"it\", \"your\", \"my\", \"this\", \"that\", \"those\", \"these\",\n",
    "\n",
    "    # Conjunctions, prepositions\n",
    "    \"and\", \"or\", \"but\", \"if\", \"because\", \"so\", \"also\", \"then\", \"when\", \"while\", \"where\", \"in\", \"on\", \"at\", \"of\",\n",
    "\n",
    "    # Noise tokens and fillers\n",
    "    \"uh\", \"um\", \"yeah\", \"okay\", \"okay.\", \"yes\", \"no\", \"thanks\", \"hello\", \"hi\", \"bye\"\n",
    "}\n",
    "\n",
    "def clean_name(raw):\n",
    "    raw = raw.lower().strip()\n",
    "    return raw if raw in VALID_NAMES else None\n",
    "\n",
    "def is_likely_name(word):\n",
    "    \"\"\"Use spaCy to check if a word is likely a proper name (Noun or pronoun)\"\"\"\n",
    "    doc = nlp(word)\n",
    "    return any(token.pos_ in {\"PROPN\", \"NOUN\"} for token in doc)\n",
    "\n",
    "def parse_transcription(text):\n",
    "    lines = []\n",
    "    for line in text.strip().split('\\n'):\n",
    "        if line.startswith('#') or '|' not in line:\n",
    "            continue\n",
    "        parts = line.split('|')\n",
    "        if len(parts) >= 4:\n",
    "            _, _, speaker, utterance = parts[:4]\n",
    "            lines.append((speaker.strip().lower(), utterance.strip()))\n",
    "    return lines\n",
    "\n",
    "def extract_names_by_max_score(lines):\n",
    "    speaker_name_scores = defaultdict(lambda: defaultdict(int))  # speaker  name  total score\n",
    "\n",
    "    for speaker, utt in lines:\n",
    "        for pattern in SELF_PATTERNS:\n",
    "            match = re.search(pattern, utt, re.IGNORECASE)\n",
    "            if match:\n",
    "                raw_name = match.group(1).strip()\n",
    "                if not is_likely_name(raw_name):\n",
    "                    # print(f\"\\t[DEBUG] Skipping {raw_name} as it is not noun\")\n",
    "                    continue\n",
    "                if raw_name in INVALID_NAME_TOKENS:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"[DEBUG] Speaker: {speaker}, Matched pattern: '{pattern}', Raw name: '{raw_name}', Utterance: '{utt}'\")\n",
    "                for valid in VALID_NAMES:\n",
    "                    score = fuzz.ratio(raw_name.lower(), valid)\n",
    "                    speaker_name_scores[speaker][valid] += score\n",
    "                break  # stop at first matching pattern\n",
    "\n",
    "    final_names = {}\n",
    "    print(speaker_name_scores)\n",
    "    for speaker, scores in speaker_name_scores.items():\n",
    "        if scores:\n",
    "            best_match = max(scores.items(), key=lambda x: x[1])\n",
    "            final_names[speaker] = best_match[0].capitalize()\n",
    "    return final_names\n",
    "\n",
    "def infer_roles_from_behavior(speaker_lines):\n",
    "    role_actions = defaultdict(Counter)\n",
    "    for speaker, utt in speaker_lines:\n",
    "        utt_lower = utt.lower()\n",
    "        if any(phrase in utt_lower for phrase in [\"i got\", \"i offer\", \"i'm offering\", \"i am offering\", \"i'll give\", \"i will give\", \"i'm giving\", 'i am giving', \"i give\", \"you rejected it\", \"you approved it\", \"you take it\", \"you took it\"]):\n",
    "            role_actions[speaker]['offer'] += 1\n",
    "        elif any(phrase in utt_lower for phrase in ['i accept', \"i'll accept\", \"i agree\", \"i am going to accept\", \"i'm going to accept\", \"i'm gonna accept\", \"i am gonna accept\"]):\n",
    "            role_actions[speaker]['accept'] += 1\n",
    "        elif any(phrase in utt_lower for phrase in ['i reject', \"i don't accept\", \"i refuse\", \"i'm gonna reject\", \"i am gonna reject\", \"i'm going to reject\", \"i am going to reject\"]):\n",
    "            print(f\"speaker {speaker} is rejecting with utternace: {utt_lower}\")\n",
    "            role_actions[speaker]['reject'] += 1\n",
    "\n",
    "    inferred_roles = {}\n",
    "    print(role_actions)\n",
    "    if role_actions:\n",
    "        offer_speaker = max(role_actions.items(), key=lambda x: x[1]['offer'])[0]\n",
    "        inferred_roles[offer_speaker] = 'Ali'\n",
    "        accept_candidates = {s: v['accept'] + v['reject'] for s, v in role_actions.items() if s != offer_speaker}\n",
    "        if accept_candidates:\n",
    "            receive_speaker = max(accept_candidates.items(), key=lambda x: x[1])[0]\n",
    "            inferred_roles[receive_speaker] = \"Rowan\"\n",
    "    return inferred_roles\n",
    "\n",
    "def detect_speakers(text):\n",
    "    speaker_lines = parse_transcription(text)\n",
    "    name_guesses = extract_names_by_max_score(speaker_lines)\n",
    "    role_guesses = infer_roles_from_behavior(speaker_lines)\n",
    "    print(role_guesses)\n",
    "    return {**role_guesses, **name_guesses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b92dcd-2d2b-4aa6-b3ef-ea9ea3c566d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'just', Utterance: 'So I'm just saying if you accept her.'\n",
      "\t[DEBUG] Skipping just as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bthis is (\\w+)', Raw name: 'just', Utterance: 'This is just like you guys talk. You accept or reject their offer. That's it.'\n",
      "\t[DEBUG] Skipping just as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bi'm (\\w+)', Raw name: 'I', Utterance: 'I Yeah, so you gotta turn your camera off Um, I figured that it would be um turned on so I was like, okay, I'll just stop. I did the same thing. I'm I'm based towards me Um, all right Rename your stuff. It's 41 Yeah, I was just doing that perfect That's good. Um, I'm gonna press record on oh I'm recording. Yeah. I forgot this You want to do'\n",
      "\t[DEBUG] Skipping I as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'Just send a request. Okay. Um, so you guys know the rules. Um, I'm going to be outside.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bi'm (\\w+)', Raw name: 'just', Utterance: 'I'm just waiting, just knock on the door when you guys are done. Okay. Do that.'\n",
      "\t[DEBUG] Skipping just as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bmy name is (\\w+)', Raw name: 'Rola', Utterance: 'Okay, so my name is Rola and what's your name?'\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bmy name is (\\w+)', Raw name: 'Ali', Utterance: 'My name is Ali.'\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'studying', Utterance: 'I'm studying HR resources.'\n",
      "\t[DEBUG] Skipping studying as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'liking', Utterance: 'I'm liking it.'\n",
      "\t[DEBUG] Skipping liking as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'currently', Utterance: 'I'm currently in third year.'\n",
      "\t[DEBUG] Skipping currently as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'more', Utterance: 'I feel like I'm more reserved like...'\n",
      "\t[DEBUG] Skipping more as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'not', Utterance: 'I'm not really proactive. I'm more of a shy person, to be honest.'\n",
      "\t[DEBUG] Skipping not as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'supposed', Utterance: 'I'm supposed to offer you money.'\n",
      "\t[DEBUG] Skipping supposed as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'offering', Utterance: 'I'm offering $5 to you accept it.'\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'gonna', Utterance: 'Okay, I'm gonna accept the offer.'\n",
      "\t[DEBUG] Skipping gonna as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'kind', Utterance: 'I'm kind of in a room alone.'\n",
      "\t[DEBUG] Skipping kind as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'in', Utterance: 'Yeah, you should probably knock bro because I'm in the I'm on the side of the TRS. So'\n",
      "\t[DEBUG] Skipping in as it is not noun\n",
      "defaultdict(<function extract_names_by_max_score.<locals>.<lambda> at 0x7f40f950a830>, {'speaker 1': defaultdict(<class 'int'>, {'ali': 29, 'rowan': 67, 'sam': 29, 'bailey': 20}), 'speaker 2': defaultdict(<class 'int'>, {'ali': 118, 'rowan': 56, 'sam': 33, 'bailey': 58})})\n",
      "Detected speaker names:\n",
      "speaker 1: Rowan\n",
      "speaker 2: Ali\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NATA41-R (UNPINNED)_full.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "lines = parse_transcription(transcript)\n",
    "speaker_names = extract_names_by_max_score(lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in speaker_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e916cc-caa1-49b9-9a38-47cbf244f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'just', Utterance: 'So I'm just saying if you accept her.'\n",
      "\t[DEBUG] Skipping just as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bthis is (\\w+)', Raw name: 'just', Utterance: 'This is just like you guys talk. You accept or reject their offer. That's it.'\n",
      "\t[DEBUG] Skipping just as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bi'm (\\w+)', Raw name: 'I', Utterance: 'I Yeah, so you gotta turn your camera off Um, I figured that it would be um turned on so I was like, okay, I'll just stop. I did the same thing. I'm I'm based towards me Um, all right Rename your stuff. It's 41 Yeah, I was just doing that perfect That's good. Um, I'm gonna press record on oh I'm recording. Yeah. I forgot this Uh'\n",
      "\t[DEBUG] Skipping I as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'Just send a request. Okay. Um, so you guys know the rules. Um, I'm going to be outside.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker_0, Matched pattern: '\\bi'm (\\w+)', Raw name: 'just', Utterance: 'I'm just waiting, just knock on the door when you guys are done. Okay. Do that.'\n",
      "\t[DEBUG] Skipping just as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bmy name is (\\w+)', Raw name: 'Rola', Utterance: 'Okay, so my name is Rola and what's your name?'\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bmy name is (\\w+)', Raw name: 'Ali', Utterance: 'My name is Ali.'\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'studying', Utterance: 'I'm studying HR resources.'\n",
      "\t[DEBUG] Skipping studying as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'studying', Utterance: 'Thanks, bro. That was good. I'm studying business.'\n",
      "\t[DEBUG] Skipping studying as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'liking', Utterance: 'I'm liking it.'\n",
      "\t[DEBUG] Skipping liking as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'currently', Utterance: 'I'm currently in third year.'\n",
      "\t[DEBUG] Skipping currently as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'more', Utterance: 'I feel like I'm more reserved like...'\n",
      "\t[DEBUG] Skipping more as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'not', Utterance: 'I'm not really proactive. I'm more of a shy person, to be honest.'\n",
      "\t[DEBUG] Skipping not as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'supposed', Utterance: 'I'm supposed to offer you money.'\n",
      "\t[DEBUG] Skipping supposed as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'offering', Utterance: 'I'm offering $5 to you accept it.'\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'gonna', Utterance: 'Okay, I'm gonna accept the offer.'\n",
      "\t[DEBUG] Skipping gonna as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'kind', Utterance: 'I'm kind of in a room alone.'\n",
      "\t[DEBUG] Skipping kind as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'in', Utterance: 'Yeah, you should probably knock bro because I'm in the I'm on the side of the TRS. So'\n",
      "\t[DEBUG] Skipping in as it is not noun\n",
      "defaultdict(<function extract_names_by_max_score.<locals>.<lambda> at 0x7f40f8aed240>, {'speaker 1': defaultdict(<class 'int'>, {'ali': 29, 'rowan': 67, 'sam': 29, 'bailey': 20}), 'speaker 2': defaultdict(<class 'int'>, {'ali': 118, 'rowan': 56, 'sam': 33, 'bailey': 58})})\n",
      "Detected speaker names:\n",
      "speaker 1: Rowan\n",
      "speaker 2: Ali\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NATA41 (SUSPECTED)_full.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "lines = parse_transcription(transcript)\n",
    "speaker_names = extract_names_by_max_score(lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in speaker_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "777b1037-57fa-4c86-af60-791ab30e6404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Speaker: speaker_2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'I'm going to send you the.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker_2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'That's fine. I'm going to turn on our video because you guys got to do the same.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker_2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'And then you guys should be able to see me. I'm going to have my project going to sit here. And then you guys can get straight.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'Ali', Utterance: 'I'm Ali.'\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'Rohan', Utterance: 'Hi, I'm Rohan.'\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'good', Utterance: 'I'm good. How are you?'\n",
      "\t[DEBUG] Skipping good as it is not noun\n",
      "[DEBUG] Speaker: speaker 2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'supposed', Utterance: 'Well, apparently you are supposed to make an offer to me and I'm supposed to accept or reject it.'\n",
      "\t[DEBUG] Skipping supposed as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'offering', Utterance: 'Okay, so I'm offering you five dollars.'\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'doing', Utterance: 'I'm doing the study so I think this so it's supposed to go I'm not really familiar with it but I'm all the time.'\n",
      "\t[DEBUG] Skipping doing as it is not noun\n",
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'I'll throw you. I'm going to throw that soon.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "defaultdict(<function extract_names_by_max_score.<locals>.<lambda> at 0x7f401d8405e0>, {'speaker 1': defaultdict(<class 'int'>, {'ali': 118, 'rowan': 56, 'sam': 33, 'bailey': 58}), 'speaker 2': defaultdict(<class 'int'>, {'ali': 25, 'rowan': 80, 'sam': 25, 'bailey': 18})})\n",
      "Detected speaker names:\n",
      "speaker 1: Ali\n",
      "speaker 2: Rowan\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NVTV25-R (SUSPECTED)_full.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "lines = parse_transcription(transcript)\n",
    "speaker_names = extract_names_by_max_score(lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in speaker_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5964ed8d-f777-4483-9689-8ab5eaaef543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Speaker: speaker 1, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'Yeah. All right. So during your interaction, I cannot be in the room. So I'm going to leave and then wait on the, like the right and front of the door. So once you're done, you can just open it and then come back and take you to. Okay.'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker_2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'offering', Utterance: 'Okay, so I was given $10 and I'm offering you five.'\n",
      "[DEBUG] Speaker: speaker_4, Matched pattern: '\\bi'm (\\w+)', Raw name: 'going', Utterance: 'I'm going to go ahead and see if I can get a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit'\n",
      "\t[DEBUG] Skipping going as it is not noun\n",
      "[DEBUG] Speaker: speaker_2, Matched pattern: '\\bi'm (\\w+)', Raw name: 'telling', Utterance: 'Yes, I'm telling the truth. I got $10 because I feel bad if I took more than that and low key I'd rather have it even than give you more so'\n",
      "\t[DEBUG] Skipping telling as it is not noun\n",
      "defaultdict(<function extract_names_by_max_score.<locals>.<lambda> at 0x7f401d840f70>, {'speaker_2': defaultdict(<class 'int'>, {'ali': 18, 'rowan': 31, 'sam': 0, 'bailey': 14})})\n",
      "Detected speaker names:\n",
      "speaker_2: Rowan\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NVTV35 (SUSPECTED)_full.txt'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "lines = parse_transcription(transcript)\n",
    "speaker_names = extract_names_by_max_score(lines)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "for speaker, name in speaker_names.items():\n",
    "    print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7015722-73e0-4d23-aaef-c26693c81f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function extract_names_by_max_score.<locals>.<lambda> at 0x7f401b10bac0>, {})\n",
      "defaultdict(<class 'collections.Counter'>, {'speaker_2': Counter({'offer': 2})})\n",
      "{'speaker_2': 'Ali'}\n",
      "Detected speaker names:\n",
      "{'speaker_2': 'Ali'}\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Test/full/NVTV35 (SUSPECTED)_full.txt'\n",
    "# file_path = 'Test/full/NATA41-R (UNPINNED)_full.txt'\n",
    "# file_path = 'Test/full/NVTV25-R (SUSPECTED)_full.txt'\n",
    "# file_path = 'Test/full/NVTV26 (SUSPECTED)_full.txt'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "detected = detect_speakers(transcript)\n",
    "\n",
    "print(\"Detected speaker names:\")\n",
    "print(detected)\n",
    "# for speaker, name in speaker_names.items():\n",
    "    # print(f\"{speaker}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd1f43-38cc-4191-bc50-402f18cfc6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transcript)",
   "language": "python",
   "name": "transcript"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
